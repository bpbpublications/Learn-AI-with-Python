{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Sentences-Word_tokenize() module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'module',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'for',\n",
       " 'basic',\n",
       " 'tokenizing',\n",
       " 'of',\n",
       " 'sentences',\n",
       " 'into',\n",
       " 'words',\n",
       " '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "word_tokenize(\"This module can be used for basic tokenizing of sentences into words.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Sentences-TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'vegan',\n",
       " 'pizza',\n",
       " 'cost',\n",
       " '$',\n",
       " '12.25',\n",
       " 'in',\n",
       " 'Pheonix',\n",
       " ',',\n",
       " 'Arizona.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them.',\n",
       " 'Thank',\n",
       " 'you',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = '''Good vegan pizza cost $12.25\\nin Pheonix, Arizona.  Please buy me\\ntwo of them.\\nThank you.'''\n",
    "TreebankWordTokenizer().tokenize(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', \"'ll\", 'save', 'and', 'invest', 'for', 'his', 'retirement', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = \"He'll save and invest for his retirement.\"\n",
    "TreebankWordTokenizer().tokenize(sentence1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'he', 'ca', \"n't\", 'go', 'to', 'market', ',']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = \"Hello, he can't go to market,\"\n",
    "TreebankWordTokenizer().tokenize(sentence2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Sentences-WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', \"'\", 'll', 'save', 'and', 'invest', 'for', 'his', 'retirement', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "sentence = \"He'll save and invest for his retirement.\"\n",
    "WordPunctTokenizer().tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Sentences-RegexpTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"He'll\", 'save', 'and', 'invest', 'for', 'his', 'retirement']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "sentence = \"He'll save and invest for his retirement.\"\n",
    "tokenizer.tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Paragraphs-sent_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It shows the difference between word tokenizer and sentence tokenizer.',\n",
       " \"It's a simple example.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"It shows the difference between word tokenizer and sentence tokenizer. It's a simple example.\"\n",
    "sent_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming-PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming_word = PorterStemmer()\n",
    "\n",
    "stemming_word.stem('writing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming_word.stem('working')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming-LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemming_Lanc = LancasterStemmer()\n",
    "\n",
    "stemming_Lanc.stem('reads')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sweet'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming_Lanc.stem('sweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming-RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enjoy'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "Regexp_stemmer = RegexpStemmer('ing')\n",
    "\n",
    "Regexp_stemmer.stem('enjoying')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enjoy'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Regexp_stemmer.stem('ingenjoy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming-SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "SnowballStemmer.languages #Languge supported by Snowball Stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bonjour'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Language_French = SnowballStemmer('french')\n",
    "\n",
    "Language_French.stem ('Bonjoura')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Language_English = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "Language_English.stem ('Eating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Language_English.stem ('Reading')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reading'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ex_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "ex_lemmatizer.lemmatize('reading')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sweet'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_lemmatizer.lemmatize('sweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implementing Stemming\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ex_wordstemmer = PorterStemmer()\n",
    "\n",
    "ex_wordstemmer.stem('believe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' believe '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implementing Lemmatization\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ex_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ex_lemmatizer.lemmatize(' believe ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "S = [(\"This\", \"DT\"),(\"book\", \"NN\"),(\"has\",\"VBZ\"),(\"ten\",\"JJ\"),(\"chapters\",\"NNS\")]\n",
    "\n",
    "chunker=nltk.RegexpParser(r''' \n",
    "NP:{<DT><NN.*><.*>*<NN.*>} \n",
    "}<VB.*>{ \n",
    "''')\n",
    "\n",
    "chunker.parse(S)\n",
    "\n",
    "Output=chunker.parse(S)\n",
    "\n",
    "Output.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words(BoW) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bag': 0, 'of': 7, 'words': 15, 'model': 5, 'is': 4, 'very': 14, 'useful': 13, 'nlp': 6, 'technique': 8, 'used': 12, 'to': 11, 'extract': 1, 'the': 10, 'features': 2, 'from': 3, 'text': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "Sentences=['Bag of Words model is very useful NLP technique.', 'Bag of Words model is used to extract the features from text.']\n",
    "\n",
    "vector_count = CountVectorizer()\n",
    "\n",
    "text_feature = vector_count.fit_transform(Sentences).todense()\n",
    "\n",
    "print(vector_count.vocabulary_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1]\n",
      " [1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(text_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example1: Predicting the Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training data: (2755, 39297)\n",
      "\n",
      "The Input Data is: Columbia is the name of a space shuttle \n",
      " Category: Space\n",
      "\n",
      "The Input Data is: Hindu, isai, Sikh, Muslim all are religions \n",
      " Category: Religion\n",
      "\n",
      "The Input Data is: We shoul drive safely \n",
      " Category: Autos\n",
      "\n",
      "The Input Data is: Puck is a round disk made of hard rubber \n",
      " Category: Hockey\n",
      "\n",
      "The Input Data is: Television, Microwave, Mixer Grinder, Refrigrator, all uses electricity \n",
      " Category: Electronics\n"
     ]
    }
   ],
   "source": [
    "#Import the required packages\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Defining five different category maps\n",
    "c_map = {'talk.religion.misc': 'Religion', 'rec.autos': 'Autos','rec.sport.hockey':'Hockey','sci.electronics':'Electronics',        'sci.space': 'Space'} \n",
    "#Creating the training set\n",
    "t_data = fetch_20newsgroups(subset='train', \n",
    "        categories=c_map.keys(), shuffle=True, random_state=5)\n",
    "#Building a count vectorizer and extracting the term counts\n",
    "v_count = CountVectorizer()\n",
    "train_tc = v_count.fit_transform(t_data.data)\n",
    "print(\"\\nDimensions of training data:\", train_tc.shape)\n",
    "#Creating tf-idf transformer\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_tc)\n",
    "#Defining the test data\n",
    "input_data = [\n",
    "    'Columbia is the name of a space shuttle',\n",
    "    'Hindu, isai, Sikh, Muslim all are religions',\n",
    "    'We shoul drive safely',\n",
    "    'Puck is a round disk made of hard rubber',\n",
    "    'Television, Microwave, Mixer Grinder, Refrigrator, all uses electricity']\n",
    "#Multimonial Naïve Bayes classifier training\n",
    "classifier = MultinomialNB().fit(train_tfidf, t_data.target)\n",
    "#Transforming input data by using count vectorizer\n",
    "input_tc = v_count.transform(input_data)\n",
    "#Transforming vectorized data by using tf-idf transformer\n",
    "input_tfidf = tfidf.transform(input_tc)\n",
    "#Predicting output categories\n",
    "predictions = classifier.predict(input_tfidf)\n",
    "for sent, category in zip(input_data, predictions):\n",
    "    print('\\nThe Input Data is:', sent, '\\n Category:', \\\n",
    "            c_map[t_data.target_names[category]])   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example2: Gender Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aarav is a male.\n",
      "Shilpi is a male.\n",
      "0.8\n",
      "Most Informative Features\n",
      "             last_letter = 'k'              male : female =     14.3 : 1.0\n",
      "             last_letter = 'o'              male : female =      5.5 : 1.0\n",
      "             last_letter = 'z'              male : female =      4.8 : 1.0\n",
      "             last_letter = 'f'              male : female =      4.3 : 1.0\n",
      "             last_letter = 'm'              male : female =      4.0 : 1.0\n",
      "             last_letter = 'j'              male : female =      3.8 : 1.0\n",
      "             last_letter = 'g'              male : female =      3.1 : 1.0\n",
      "             last_letter = 'r'              male : female =      2.9 : 1.0\n",
      "             last_letter = 'p'              male : female =      2.6 : 1.0\n",
      "             last_letter = 'd'              male : female =      2.6 : 1.0\n",
      "             last_letter = 'v'              male : female =      2.1 : 1.0\n",
      "             last_letter = 'u'              male : female =      2.1 : 1.0\n",
      "             last_letter = 's'              male : female =      2.0 : 1.0\n",
      "             last_letter = 'n'              male : female =      1.9 : 1.0\n",
      "             last_letter = 'e'            female : male   =      1.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Import the required packages\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "names_M = [(name, 'male') for name in names.words(r\"C:/Users/Leekha/Desktop/malenames.txt\")]\n",
    "names_F = [(name, 'female') for name in names.words(r\"C:/Users/Leekha/Desktop/femalenames.txt\")]\n",
    "names_labels = names_M + names_F\n",
    "random.shuffle(names_labels)\n",
    "#Defining the function to calculate features\n",
    "def features(word):\n",
    "      return {'last_letter': word[-1]}\n",
    "featuresets = [(features(n), gender) for (n, gender) in names_labels]\n",
    "# Splitting  the dataset into training set and testing set.\n",
    "train_set, test_set = featuresets[5:], featuresets[:5]\n",
    "# Training the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "male_gender = classifier.classify(features('Aarav'))\n",
    "female_gender = classifier.classify(features('Shilpi'))\n",
    "print(\"Aarav is a {}.\".format(male_gender))\n",
    "print(\"Shilpi is a {}.\".format(female_gender))\n",
    "#Getting the accuracy\n",
    "print(accuracy(classifier, test_set))\n",
    "#Printing first 15 feature sets\n",
    "classifier.show_most_informative_features(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
